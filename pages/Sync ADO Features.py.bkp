# pages/Sync ADO Features.py
import io
import uuid
from datetime import date
from typing import Optional, List, Dict, Tuple, Any

import pandas as pd
import streamlit as st

from snowflake_db import (
    execute,
    fetch_df,
    ensure_ado_minimal_tables,
    ensure_team_calc_table,
    ensure_team_cost_view,
    repair_ado_effort_points_precision,

    # Lookups + upserts (align with your schema)
    list_programs,
    list_teams,
    list_vendors,
    list_application_groups,
    list_applications,
    upsert_program,
    upsert_team,
    upsert_vendor,
    upsert_application_group,
    upsert_application_instance,
    upsert_invoice,
)

# -------------------------
# Page setup
# -------------------------
st.set_page_config(page_title="Sync ADO Features", layout="wide")
st.title("🔄 Sync ADO Features (XLSX‑friendly, persistent upload)")

# Ensure minimal schema is ready
with st.spinner("Ensuring minimal ADO schema..."):
    ensure_ado_minimal_tables()
    if callable(ensure_team_calc_table):
        ensure_team_calc_table()
    if callable(ensure_team_cost_view):
        ensure_team_cost_view()

# -------------------------
# Session state
# -------------------------
for key, default in [
    ("ado_parsed_raw", None),
    ("ado_parsed_norm", None),
    ("one_sheet_df", None),
    ("colmap", {}),
    ("previews", {}),
]:
    if key not in st.session_state:
        st.session_state[key] = default

# -------------------------
# Column expectations (ADO)
# -------------------------
EXPECTED = {
    "Effort": ["Effort", "Story Points", "Effort Points", "EFFORT_POINTS"],
    "Team": ["Team", "System.Team", "Area Team"],
    "Custom_ApplicationName": ["Custom_ApplicationName", "Application", "App Name"],
    "Iteration": ["Iteration", "Iteration Path", "System.IterationPath", "Iteration.IterationLevel3.2"],
    "Title": ["Title", "System.Title"],
    "State": ["State", "System.State"],
    "ID": ["ID", "Work Item ID", "System.Id", "WorkItemId", "Work Item Id"],
    "CreatedDate": ["Created Date", "System.CreatedDate", "CreatedDate"],
    "ChangedDate": ["Changed Date", "System.ChangedDate", "ChangedDate"],
    "Year": ["Year", "ADO Year", "ADO_YEAR"],
}

# -------------------------
# Utilities (file parsing)
# -------------------------
def _auto_header_index(df_no_header: pd.DataFrame, expected_samples: List[str], max_scan: int = 20) -> Optional[int]:
    for i in range(min(max_scan, len(df_no_header))):
        row_vals = df_no_header.iloc[i].astype(str).str.strip().str.lower().tolist()
        hits = sum(1 for e in expected_samples if e.lower() in row_vals)
        if hits >= 2:
            return i
    return None

def _list_excel_sheets(data: bytes) -> List[str]:
    sheets: List[str] = []
    try:
        import openpyxl
        wb = openpyxl.load_workbook(io.BytesIO(data), read_only=True)
        return list(wb.sheetnames)
    except Exception:
        pass
    try:
        import xlrd
        wb = xlrd.open_workbook(file_contents=data)
        return wb.sheet_names()
    except Exception:
        pass
    try:
        from pyxlsb import open_workbook
        with open_workbook(fileobj=io.BytesIO(data)) as wb:
            return [s.name for s in wb.sheets]
    except Exception:
        pass
    return sheets

def _read_excel_any(data: bytes, sheet_name: Optional[str], diag: Dict) -> Optional[pd.DataFrame]:
    errors = []
    for eng in ("openpyxl", "xlrd", "pyxlsb"):
        try:
            __import__(eng)
            df = pd.read_excel(io.BytesIO(data), sheet_name=(sheet_name or 0), engine=eng)
            diag.setdefault("excel_engines_used", []).append(eng)
            return df
        except ModuleNotFoundError as e:
            errors.append(f"{eng} not installed: {e}")
        except Exception as e:
            errors.append(f"{eng} failed: {e}")
    try:
        df = pd.read_excel(io.BytesIO(data), sheet_name=(sheet_name or 0))
        diag.setdefault("excel_engines_used", []).append("auto")
        return df
    except Exception as e:
        errors.append(f"pandas auto engine failed: {e}")
    try:
        df_raw = pd.read_excel(io.BytesIO(data), sheet_name=(sheet_name or 0), header=None)
        hi = _auto_header_index(df_raw, expected_samples=["Title", "ID", "Team", "Iteration", "State", "Effort"])
        if hi is not None:
            df = pd.read_excel(io.BytesIO(data), sheet_name=(sheet_name or 0), header=hi)
            diag.setdefault("header_autodetected", True)
            return df
        else:
            errors.append("Header auto-detect failed.")
    except Exception as e:
        errors.append(f"header=None strategy failed: {e}")

    diag["excel_errors"] = errors
    return None

def _read_csv_any(data: bytes, diag: Dict) -> Optional[pd.DataFrame]:
    encodings: List[str] = ["utf-8-sig", "utf-8", "cp1252", "latin-1", "utf-16", "utf-16le", "utf-16be"]
    seps: List[Optional[str]] = [",", ";", "\t", None]
    errors = []
    for enc in encodings:
        for sep in seps:
            try:
                df = pd.read_csv(io.BytesIO(data), encoding=enc, sep=sep, engine="python")
                if df.shape[1] >= 2:
                    diag.setdefault("csv_attempts", []).append({"encoding": enc, "sep": sep or "auto"})
                    return df
            except Exception as e:
                errors.append(f"csv {enc}/{sep or 'auto'} failed: {e}")
    diag["csv_errors"] = errors
    return None

def _read_file_any(upl, sheet_name: Optional[str], diag: Dict) -> pd.DataFrame:
    if upl is None:
        raise ValueError("No file uploaded.")
    upl.seek(0)
    raw = upl.read()
    upl.seek(0)
    name_lower = (upl.name or "").lower()
    looks_like_excel = any(ext in name_lower for ext in (".xlsx", ".xlsm", ".xls", ".xlsb")) or (b"\x00" in raw)

    df: Optional[pd.DataFrame] = None
    if looks_like_excel:
        df = _read_excel_any(raw, sheet_name, diag)
        if df is None:
            df = _read_csv_any(raw, diag)
    else:
        df = _read_csv_any(raw, diag)
        if df is None:
            df = _read_excel_any(raw, sheet_name, diag)

    if df is None or df.empty:
        raise ValueError("Could not parse the file. Try a clean XLSX (preferred) or CSV UTF‑8.")
    # Normalize column header BOM/whitespace
    df.columns = [str(c).strip().replace("\ufeff", "") for c in df.columns]
    return df

# -------------------------
# ADO parsing (unchanged core)
# -------------------------
def _auto_rename_columns(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df.columns = [str(c).strip().replace("\ufeff", "") for c in df.columns]
    rename: Dict[str, str] = {}
    for canonical, candidates in EXPECTED.items():
        # exact case-insensitive match
        for c in candidates:
            match = [col for col in df.columns if col.lower() == c.lower()]
            if match:
                rename[match[0]] = canonical
                break
        else:
            match2 = [col for col in df.columns if canonical.lower() in col.lower()]
            if match2:
                rename[match2[0]] = canonical
    return df.rename(columns=rename)

def read_ado_upload_any(upl, sheet_name: Optional[str], effort_uses_comma: bool, diag: Dict) -> pd.DataFrame:
    if upl is None:
        raise ValueError("No file uploaded.")
    df = _read_file_any(upl, sheet_name, diag)
    df = _auto_rename_columns(df)

    for col in ("Team", "Custom_ApplicationName", "Iteration", "Title", "State"):
        if col in df.columns:
            df[col] = df[col].astype(str).str.strip()

    if "Effort" in df.columns:
        if effort_uses_comma:
            df["Effort"] = df["Effort"].astype(str).str.replace(",", ".", regex=False)
        df["Effort"] = pd.to_numeric(df["Effort"], errors="coerce")

    if "ID" in df.columns:
        df["ID"] = df["ID"].astype(str).str.strip()

    for dcol in ("CreatedDate", "ChangedDate"):
        if dcol in df.columns:
            df[dcol] = pd.to_datetime(df[dcol], errors="coerce")

    if "Year" in df.columns:
        df["Year"] = df["Year"].astype(str).str.replace(",", ".", regex=False)
        df["Year"] = pd.to_numeric(df["Year"], errors="coerce").astype("Int64")

    return df

def normalize_to_canonical(df: pd.DataFrame) -> pd.DataFrame:
    out = pd.DataFrame()
    out["FEATURE_ID"]     = df.get("ID", pd.Series(dtype=str)).astype(str)
    out["TITLE"]          = df.get("Title")
    out["STATE"]          = df.get("State")
    out["TEAM_RAW"]       = df.get("Team")
    out["APP_NAME_RAW"]   = df.get("Custom_ApplicationName")
    out["EFFORT_POINTS"]  = df.get("Effort")
    out["ITERATION_PATH"] = df.get("Iteration")
    out["CREATED_AT"]     = df.get("CreatedDate")
    out["CHANGED_AT"]     = df.get("ChangedDate")
    out["ADO_YEAR"]       = df.get("Year")

    out["EFFORT_POINTS"]  = pd.to_numeric(out["EFFORT_POINTS"], errors="coerce")
    out["CREATED_AT"]     = pd.to_datetime(out["CREATED_AT"], errors="coerce")
    out["CHANGED_AT"]     = pd.to_datetime(out["CHANGED_AT"], errors="coerce")
    try:
        out["ADO_YEAR"] = pd.to_numeric(out["ADO_YEAR"], errors="coerce").astype("Int64")
    except Exception:
        pass

    out = out[~out["FEATURE_ID"].isna() & (out["FEATURE_ID"].astype(str).str.len() > 0)].copy()
    return out.reset_index(drop=True)

def _to_py(v):
    import pandas as _pd, numpy as _np
    if v is None:
        return None
    if isinstance(v, _pd._libs.tslibs.nattype.NaTType):
        return None
    if isinstance(v, _pd.Timestamp):
        return v.to_pydatetime()
    if _pd.isna(v):
        return None
    if isinstance(v, _np.floating):
        return None if _np.isnan(v) else float(v)
    if isinstance(v, _np.integer):
        return int(v)
    if isinstance(v, str):
        s = v.strip()
        return s if s != "" else None
    return v

def upsert_ado_features(df: pd.DataFrame) -> int:
    if df.empty:
        return 0
    try:
        execute("ALTER TABLE ADO_FEATURES ADD COLUMN IF NOT EXISTS ADO_YEAR NUMBER(4)")
    except Exception:
        pass

    rows: List[Tuple] = []
    for _, r in df.iterrows():
        rows.append((
            _to_py(r.get("FEATURE_ID")),
            _to_py(r.get("TITLE")),
            _to_py(r.get("STATE")),
            _to_py(r.get("TEAM_RAW")),
            _to_py(r.get("APP_NAME_RAW")),
            _to_py(r.get("EFFORT_POINTS")),
            _to_py(r.get("ITERATION_PATH")),
            _to_py(r.get("CREATED_AT")),
            _to_py(r.get("CHANGED_AT")),
            _to_py(r.get("ADO_YEAR")),
        ))
    rows = [t for t in rows if t[0] is not None]
    if not rows:
        return 0

    sql = """
    MERGE INTO ADO_FEATURES t
    USING (
      SELECT %s AS FEATURE_ID, %s AS TITLE, %s AS STATE,
             %s AS TEAM_RAW, %s AS APP_NAME_RAW, %s AS EFFORT_POINTS,
             %s AS ITERATION_PATH, %s AS CREATED_AT, %s AS CHANGED_AT,
             %s AS ADO_YEAR
    ) s
    ON t.FEATURE_ID = s.FEATURE_ID
    WHEN MATCHED THEN UPDATE SET
      TITLE = s.TITLE,
      STATE = s.STATE,
      TEAM_RAW = s.TEAM_RAW,
      APP_NAME_RAW = s.APP_NAME_RAW,
      EFFORT_POINTS = s.EFFORT_POINTS,
      ITERATION_PATH = s.ITERATION_PATH,
      CREATED_AT = s.CREATED_AT,
      CHANGED_AT = s.CHANGED_AT,
      ADO_YEAR = s.ADO_YEAR
    WHEN NOT MATCHED THEN INSERT
      (FEATURE_ID, TITLE, STATE, TEAM_RAW, APP_NAME_RAW, EFFORT_POINTS, ITERATION_PATH, CREATED_AT, CHANGED_AT, ADO_YEAR)
    VALUES
      (s.FEATURE_ID, s.TITLE, s.STATE, s.TEAM_RAW, s.APP_NAME_RAW, s.EFFORT_POINTS, s.ITERATION_PATH, s.CREATED_AT, s.CHANGED_AT, s.ADO_YEAR)
    """
    execute(sql, rows, many=True)
    return len(rows)

def load_ado_distincts() -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    teams = fetch_df("""
        SELECT DISTINCT TEAM_RAW
        FROM ADO_FEATURES
        WHERE TEAM_RAW IS NOT NULL AND TRIM(TEAM_RAW) <> ''
        ORDER BY TEAM_RAW
    """)
    apps  = fetch_df("""
        SELECT DISTINCT APP_NAME_RAW
        FROM ADO_FEATURES
        WHERE APP_NAME_RAW IS NOT NULL AND TRIM(APP_NAME_RAW) <> ''
        ORDER BY APP_NAME_RAW
    """)
    iters = fetch_df("""
        SELECT DISTINCT ITERATION_PATH
        FROM ADO_FEATURES
        WHERE ITERATION_PATH IS NOT NULL AND TRIM(ITERATION_PATH) <> ''
        ORDER BY ITERATION_PATH
    """)
    return teams, apps, iters

def ado_features_base_query(where_sql: str = "", params: Optional[tuple] = None) -> pd.DataFrame:
    sql = f"""
      SELECT FEATURE_ID, TITLE, TEAM_RAW, APP_NAME_RAW, EFFORT_POINTS, ITERATION_PATH, CHANGED_AT
      FROM ADO_FEATURES
      {where_sql}
      ORDER BY COALESCE(CHANGED_AT, TO_TIMESTAMP_NTZ('1900-01-01')) DESC, FEATURE_ID
    """
    df = fetch_df(sql, params)
    if "EFFORT_POINTS" in df.columns:
        df["EFFORT_POINTS"] = pd.to_numeric(df["EFFORT_POINTS"], errors="coerce")
    return df

# -------------------------
# Tabs (top-level)
# -------------------------
tab_load, tab_map, tab_explore, tab_recon, tab_bulk = st.tabs([
    "📥 Load Data", "🧭 Map Values", "🔎 ADO Explorer", "📊 Reconciliation", "📦 Load Invoices & Teams"
])

# =========================
# Tab: Load Data (ADO)
# =========================
with tab_load:
    try:
        info = fetch_df("SELECT COUNT(*) AS N FROM ADO_FEATURES")
        n_rows = int(info.iloc[0]["N"]) if not info.empty else 0
        st.info(f"Current rows in ADO_FEATURES: **{n_rows}**")
    except Exception:
        st.warning("Could not query ADO_FEATURES row count (check connection/secrets).")

    st.caption("Upload **XLSX/XLSM/XLSB/XLS** or **CSV**. Only raw ADO fields are stored (including Year if present).")
    upl = st.file_uploader("Upload ADO export", type=["xlsx", "xlsm", "xlsb", "xls", "csv"], key="upl_ado")

    sheet_name: Optional[str] = None
    effort_uses_comma = False

    if upl:
        upl.seek(0)
        file_bytes = upl.read()
        upl.seek(0)

        sheets = _list_excel_sheets(file_bytes)
        if sheets:
            sheet_name = st.selectbox("Worksheet", sheets, index=0, help="Choose the tab to import", key="sheet_select")
        else:
            st.info("Could not list sheets. I will try to read the first sheet automatically (or parse as CSV).")

        effort_uses_comma = st.checkbox(
            "Effort uses comma as decimal (e.g., 1,5)",
            value=False,
            help="Enable if your Effort column has commas instead of dots.",
            key="effort_comma"
        )

        diag: Dict = {}
        if st.button("📄 Parse file", key="btn_parse"):
            upl.seek(0)
            try:
                df_raw = read_ado_upload_any(upl, sheet_name, effort_uses_comma, diag)
            except Exception as e:
                with st.expander("Diagnostics", expanded=True):
                    st.write("**Why it failed**")
                    st.exception(e)
                    st.write("**Parse attempts**")
                    st.json(diag)
                st.error("Could not parse the ADO file. Please install Excel engines (openpyxl/xlrd/pyxlsb) or re‑export as clean XLSX/CSV UTF‑8.")
                st.stop()

            df_norm = normalize_to_canonical(df_raw)
            st.session_state["ado_parsed_raw"] = df_raw
            st.session_state["ado_parsed_norm"] = df_norm

            st.success(f"Parsed {len(df_raw)} rows from {upl.name} and normalized {len(df_norm)} rows.")
            with st.expander("Preview (raw + normalized)"):
                show_all = st.checkbox("Show ALL rows (may be large)", value=False, key="show_all_rows")
                preview = pd.concat(
                    [df_raw.reset_index(drop=True), df_norm.reset_index(drop=True)],
                    axis=1
                )
                st.dataframe((preview if show_all else preview.head(300)), use_container_width=True, height=600)

    if st.session_state["ado_parsed_norm"] is not None:
        if st.button("⬆️ Upsert into ADO_FEATURES", type="primary", key="btn_upsert"):
            with st.spinner("Loading into Snowflake..."):
                n = upsert_ado_features(st.session_state["ado_parsed_norm"])
            info2 = fetch_df("SELECT COUNT(*) AS N FROM ADO_FEATURES")
            n_rows2 = int(info2.iloc[0]["N"]) if not info2.empty else 0
            st.success(f"Upserted **{n}** rows into ADO_FEATURES. New total rows: **{n_rows2}**.")
    else:
        st.caption("Parse a file first to enable upsert.")

# =========================
# Tab: Map Values
# =========================
with tab_map:
    st.subheader("Map ADO values to TCO (no calculations)")

    cnt = fetch_df("SELECT COUNT(*) AS N FROM ADO_FEATURES")
    current_n = int(cnt.iloc[0]["N"]) if not cnt.empty else 0
    if current_n == 0:
        st.warning("ADO_FEATURES is empty. Load and upsert data in the first tab.")
    else:
        st.info(f"ADO_FEATURES currently has **{current_n}** rows.")

    ado_teams = fetch_df("""
        SELECT DISTINCT TEAM_RAW
        FROM ADO_FEATURES
        WHERE TEAM_RAW IS NOT NULL AND TEAM_RAW <> ''
        ORDER BY TEAM_RAW
    """)
    ado_apps = fetch_df("""
        SELECT DISTINCT APP_NAME_RAW
        FROM ADO_FEATURES
        WHERE APP_NAME_RAW IS NOT NULL AND APP_NAME_RAW <> ''
        ORDER BY APP_NAME_RAW
    """)

    team_maps = fetch_df("SELECT ADO_TEAM, TEAMID FROM MAP_ADO_TEAM_TO_TCO_TEAM ORDER BY ADO_TEAM")
    app_maps  = fetch_df("SELECT ADO_APP, APP_GROUP FROM MAP_ADO_APP_TO_TCO_GROUP ORDER BY ADO_APP")

    teams_df  = fetch_df("SELECT TEAMID, TEAMNAME FROM TEAMS ORDER BY TEAMNAME")
    groups_df = fetch_df("SELECT GROUPID, GROUPNAME FROM APPLICATION_GROUPS ORDER BY GROUPNAME")

    st.markdown("### ADO → TCO Team")
    if ado_teams.empty or teams_df.empty:
        st.info("Load features and create Teams first.")
    else:
        base_tm = ado_teams.rename(columns={"TEAM_RAW": "ADO_TEAM"}).copy()
        if not team_maps.empty:
            base_tm = base_tm.merge(team_maps, how="left", on="ADO_TEAM")
        else:
            base_tm["TEAMID"] = None

        id_to_name_team = {r.TEAMID: r.TEAMNAME for _, r in teams_df.iterrows()}
        name_to_id_team = {r.TEAMNAME: r.TEAMID for _, r in teams_df.iterrows()}
        base_tm["TCO_TEAMNAME"] = base_tm["TEAMID"].map(id_to_name_team)

        edited = st.data_editor(
            base_tm[["ADO_TEAM","TCO_TEAMNAME"]],
            use_container_width=True,
            height=360,
            num_rows="fixed",
            column_config={
                "ADO_TEAM": st.column_config.TextColumn("ADO Team", disabled=True),
                "TCO_TEAMNAME": st.column_config.SelectboxColumn(
                    "TCO Team",
                    options=teams_df["TEAMNAME"].tolist(),
                    required=False,
                ),
            },
            key="tm_editor",
        )
        if st.button("💾 Save Team Mappings", key="btn_save_team_mappings"):
            rows = []
            for _, r in edited.iterrows():
                ado_val = str(r["ADO_TEAM"]).strip()
                tname = r.get("TCO_TEAMNAME")
                if not ado_val:
                    continue
                if tname and tname in name_to_id_team:
                    rows.append((ado_val, name_to_id_team[tname]))
                else:
                    execute("DELETE FROM MAP_ADO_TEAM_TO_TCO_TEAM WHERE ADO_TEAM = %s", (ado_val,))
            if rows:
                merge_sql = """
                MERGE INTO MAP_ADO_TEAM_TO_TCO_TEAM t
                USING (SELECT %s AS ADO_TEAM, %s AS TEAMID) s
                ON t.ADO_TEAM = s.ADO_TEAM
                WHEN MATCHED THEN UPDATE SET TEAMID = s.TEAMID
                WHEN NOT MATCHED THEN INSERT (ADO_TEAM, TEAMID) VALUES (s.ADO_TEAM, s.TEAMID)
                """
                execute(merge_sql, rows, many=True)
            st.success("Team mappings saved.")

    st.markdown("---")

    st.markdown("### ADO → TCO App Group")
    if ado_apps.empty or groups_df.empty:
        st.info("Load features and create Application Groups first.")
    else:
        base_am = ado_apps.rename(columns={"APP_NAME_RAW": "ADO_APP"}).copy()
        if not app_maps.empty:
            base_am = base_am.merge(app_maps, how="left", on="ADO_APP")
        else:
            base_am["APP_GROUP"] = None

        id_to_name_group = {r.GROUPID: r.GROUPNAME for _, r in groups_df.iterrows()}
        name_to_id_group = {r.GROUPNAME: r.GROUPID for _, r in groups_df.iterrows()}
        base_am["TCO_GROUPNAME"] = base_am["APP_GROUP"].map(id_to_name_group)

        edited2 = st.data_editor(
            base_am[["ADO_APP","TCO_GROUPNAME"]],
            use_container_width=True,
            height=360,
            num_rows="fixed",
            column_config={
                "ADO_APP": st.column_config.TextColumn("ADO App Name", disabled=True),
                "TCO_GROUPNAME": st.column_config.SelectboxColumn(
                    "TCO App Group",
                    options=groups_df["GROUPNAME"].tolist(),
                    required=False,
                    help="This picker shows group names. The mapping will store the group ID."
                ),
            },
            key="am_editor",
        )
        if st.button("💾 Save App Group Mappings", key="btn_save_app_group_mappings"):
            rows = []
            for _, r in edited2.iterrows():
                ado_val = str(r["ADO_APP"]).strip()
                gname = r.get("TCO_GROUPNAME")
                if not ado_val:
                    continue
                if gname and gname in name_to_id_group:
                    gid = name_to_id_group[gname]
                    rows.append((ado_val, gid))
                else:
                    execute("DELETE FROM MAP_ADO_APP_TO_TCO_GROUP WHERE ADO_APP = %s", (ado_val,))
            if rows:
                merge_sql = """
                MERGE INTO MAP_ADO_APP_TO_TCO_GROUP t
                USING (SELECT %s AS ADO_APP, %s AS APP_GROUP) s
                ON t.ADO_APP = s.ADO_APP
                WHEN MATCHED THEN UPDATE SET APP_GROUP = s.APP_GROUP
                WHEN NOT MATCHED THEN INSERT (ADO_APP, APP_GROUP) VALUES (s.ADO_APP, s.APP_GROUP)
                """
                execute(merge_sql, rows, many=True)
            st.success("App Group mappings saved.")

# =========================
# Tab: 🔎 ADO Explorer
# =========================
with tab_explore:
    st.subheader("What’s coming from ADO (raw import)")
    cfix, _ = st.columns([1, 5])
    with cfix:
        if callable(repair_ado_effort_points_precision):
            if st.button("🧹 Repair Effort Points Precision", key="btn_repair_effort_explorer"):
                try:
                    repair_ado_effort_points_precision()
                    st.success("Effort points precision repaired.")
                    st.rerun()
                except Exception as e:
                    st.error(f"Repair failed: {e}")
        else:
            st.caption("Repair Effort Points action unavailable (helper not found).")

    df_teams, df_apps, df_iters = load_ado_distincts()
    c1, c2, c3 = st.columns(3)
    c1.metric("Distinct ADO Teams", len(df_teams))
    c2.metric("Distinct ADO Apps", len(df_apps))
    c3.metric("Distinct Iterations", len(df_iters))

    st.markdown("#### Filter")
    f1, f2, f3 = st.columns([2,2,2])
    team_like = f1.text_input("Team contains", "", key="exp_team_contains")
    app_like  = f2.text_input("App contains", "", key="exp_app_contains")
    iter_like = f3.text_input("Iteration contains", "", key="exp_iter_contains")

    where = []
    params: list = []
    if team_like.strip():
        where.append("UPPER(TEAM_RAW) LIKE UPPER(%s)")
        params.append(f"%{team_like.strip()}%")
    if app_like.strip():
        where.append("UPPER(APP_NAME_RAW) LIKE UPPER(%s)")
        params.append(f"%{app_like.strip()}%")
    if iter_like.strip():
        where.append("UPPER(ITERATION_PATH) LIKE UPPER(%s)")
        params.append(f"%{iter_like.strip()}%")
    where_sql = " WHERE " + " AND ".join(where) if where else ""

    st.markdown("#### Latest Features from ADO")
    df_raw = ado_features_base_query(where_sql, tuple(params) if params else None)
    st.dataframe(df_raw, use_container_width=True, height=340)

    st.markdown("#### Effort Points by Iteration (raw path)")
    df_iter_sum = fetch_df(f"""
      SELECT ITERATION_PATH,
             SUM(COALESCE(EFFORT_POINTS,0)) AS EFFORT_POINTS_SUM,
             COUNT(*) AS FEATURES
      FROM ADO_FEATURES
      {where_sql}
      GROUP BY ITERATION_PATH
      ORDER BY ITERATION_PATH
    """, tuple(params) if params else None)
    st.dataframe(df_iter_sum, use_container_width=True, height=240)

    st.markdown("#### Effort Points by Year & Iteration (Excel-based Year + parsed Iteration)")
    df_year_iter = fetch_df(f"""
      WITH base AS (
        SELECT ADO_YEAR, ITERATION_PATH, EFFORT_POINTS
        FROM ADO_FEATURES
        {where_sql}
      ),
      labeled AS (
        SELECT
          ADO_YEAR,
          COALESCE(
            REGEXP_SUBSTR(ITERATION_PATH, 'I[[:space:]]*([0-9]+)', 1, 1, 'i', 1),
            REGEXP_SUBSTR(ITERATION_PATH, 'ITERATION[[:space:]]*([0-9]+)', 1, 1, 'i', 1)
          ) AS ITER_NUM,
          EFFORT_POINTS
        FROM base
      )
      SELECT
        ADO_YEAR AS YEAR,
        CASE WHEN ITER_NUM IS NOT NULL THEN 'I' || ITER_NUM ELSE NULL END AS ITERATION,
        SUM(COALESCE(EFFORT_POINTS,0)) AS EFFORT_POINTS_SUM,
        COUNT(*) AS FEATURES
      FROM labeled
      GROUP BY ADO_YEAR, ITER_NUM
      ORDER BY YEAR, TRY_TO_NUMBER(ITER_NUM)
    """, tuple(params) if params else None)
    st.dataframe(df_year_iter, use_container_width=True, height=300)

    st.markdown("#### Effort Points by Team & Iteration (raw path)")
    df_team_iter = fetch_df(f"""
      SELECT TEAM_RAW, ITERATION_PATH,
             SUM(COALESCE(EFFORT_POINTS,0)) AS EFFORT_POINTS_SUM,
             COUNT(*) AS FEATURES
      FROM ADO_FEATURES
      {where_sql}
      GROUP BY TEAM_RAW, ITERATION_PATH
      ORDER BY TEAM_RAW, ITERATION_PATH
    """, tuple(params) if params else None)
    st.dataframe(df_team_iter, use_container_width=True, height=300)

# =========================================================
# 📊 Reconciliation
# =========================================================
with tab_recon:
    st.subheader("Reconciliation: mappings + effort + calculated costs")
    st.caption("TEAM_COST_PERPI is split equally across all features within each Team × Year × Iteration.")

    colA, colB, colC = st.columns(3)
    try:
        unmapped_team = fetch_df("""
          SELECT TEAM_RAW, COUNT(*) AS N
          FROM VW_TEAM_COSTS_PER_FEATURE v
          WHERE v.TEAMID IS NULL
          GROUP BY TEAM_RAW
          ORDER BY N DESC
        """)
        colA.metric("Features missing TEAM mapping", int(unmapped_team["N"].sum()) if not unmapped_team.empty else 0)
    except Exception as e:
        colA.warning(f"Unmapped calc failed: {e}")
        unmapped_team = pd.DataFrame()

    try:
        zero_denom = fetch_df("""
          SELECT COUNT(*) AS N
          FROM VW_TEAM_COSTS_PER_FEATURE
          WHERE (TEAMID IS NOT NULL) AND
                (COALESCE(DELIVERY_TEAM_FTE,0) + COALESCE(CONTRACTOR_CS_FTE,0) + COALESCE(CONTRACTOR_C_FTE,0)) = 0
        """)
        colB.metric("Rows with zero composition denominator", int(zero_denom.iloc[0]["N"]) if not zero_denom.empty else 0)
    except Exception as e:
        colB.warning(f"Zero-denom check failed: {e}")

    try:
        no_rate = fetch_df("""
          SELECT COUNT(*) AS N FROM VW_TEAM_COSTS_PER_FEATURE
          WHERE (TEAMID IS NOT NULL) AND
                (COALESCE(XOM_RATE,0)=0 OR COALESCE(CONTRACTOR_CS_RATE,0)=0 OR COALESCE(CONTRACTOR_C_RATE,0)=0)
        """)
        colC.metric("Rows with a missing rate", int(no_rate.iloc[0]["N"]) if not no_rate.empty else 0)
    except Exception as e:
        colC.warning(f"Rate check failed: {e}")

    st.markdown("#### Filters")
    fc1, fc2, fc3 = st.columns([2,2,2])
    team_like2 = fc1.text_input("TCO Team contains", "", key="recon_team_contains")
    app_like2  = fc2.text_input("ADO App contains", "", key="recon_app_contains")
    iter_like2 = fc3.text_input("Iteration contains", "", key="recon_iter_contains")

    where2 = []
    params2: list = []
    if team_like2.strip():
        where2.append("UPPER(TEAMNAME) LIKE UPPER(%s)")
        params2.append(f"%{team_like2.strip()}%")
    if app_like2.strip():
        where2.append("UPPER(APP_NAME_RAW) LIKE UPPER(%s)")
        params2.append(f"%{app_like2.strip()}%")
    if iter_like2.strip():
        where2.append("UPPER(ITERATION_PATH) LIKE UPPER(%s)")
        params2.append(f"%{iter_like2.strip()}%")
    where_sql2 = " WHERE " + " AND ".join(where2) if where2 else ""

    try:
        base_sql = f"""
        WITH v AS (
          SELECT
            TEAMNAME, TEAMID,
            COALESCE(ADO_YEAR, YEAR(COALESCE(CHANGED_AT, CREATED_AT))) AS ADO_YEAR,
            COALESCE(ITERATION_NUM, TRY_TO_NUMBER(REGEXP_SUBSTR(ITERATION_PATH,'I[[:space:]]*([0-9]+)',1,1,'i',1))) AS ITERATION_NUM,
            ITERATION_PATH,
            FEATURE_ID, TITLE, APP_NAME_RAW, EFFORT_POINTS,
            TEAMFTE, XOM_RATE,
            DELIVERY_TEAM_FTE, CONTRACTOR_CS_FTE, CONTRACTOR_C_FTE,
            TEAM_COST_PERPI,
            DEL_TEAM_COST_PERPI,
            TEAM_CONTRACTOR_CS_COST_PERPI,
            TEAM_CONTRACTOR_C_COST_PERPI
          FROM VW_TEAM_COSTS_PER_FEATURE
          {where_sql2}
        ),
        tp AS (
          SELECT
            TEAMID, ADO_YEAR, ITERATION_NUM,
            COUNT(*) AS FEATURES_IN_PI,
            MAX(COALESCE(TEAMFTE,0) * COALESCE(XOM_RATE,0) / 4.0) AS TEAM_PI_FIXED_COST
          FROM v
          GROUP BY TEAMID, ADO_YEAR, ITERATION_NUM
        )
        SELECT
          v.*,
          tp.FEATURES_IN_PI,
          tp.TEAM_PI_FIXED_COST,
          CASE WHEN tp.FEATURES_IN_PI > 0
               THEN tp.TEAM_PI_FIXED_COST / tp.FEATURES_IN_PI
               ELSE 0
          END AS TEAM_COST_PERPI_EQSPLIT
        FROM v
        LEFT JOIN tp
          ON tp.TEAMID = v.TEAMID
         AND tp.ADO_YEAR = v.ADO_YEAR
         AND tp.ITERATION_NUM = v.ITERATION_NUM
        ORDER BY v.TEAMNAME, v.ADO_YEAR, v.ITERATION_NUM, v.FEATURE_ID
        """
        df_calc = fetch_df(base_sql, tuple(params2) if params2 else None)
    except Exception as e:
        st.error(f"Could not read VW_TEAM_COSTS_PER_FEATURE: {e}")
        df_calc = pd.DataFrame()

    if df_calc.empty:
        st.info("No rows found with the current filters.")
        st.stop()

    if "TEAM_COST_PERPI_EQSPLIT" in df_calc.columns:
        df_calc["TEAM_COST_PERPI"] = pd.to_numeric(df_calc["TEAM_COST_PERPI_EQSPLIT"], errors="coerce").fillna(0.0)

    st.markdown("#### Effort & Cost by Team, Year & Iteration")
    ag = df_calc.groupby(["TEAMNAME","ADO_YEAR","ITERATION_NUM"], dropna=False).agg(
        FEATURES=("FEATURE_ID","count"),
        EFFORT_POINTS_SUM=("EFFORT_POINTS","sum"),
        TEAM_COST_PERPI=("TEAM_COST_PERPI","sum"),
        DEL_TEAM_COST_PERPI=("DEL_TEAM_COST_PERPI","sum"),
        TEAM_CONTRACTOR_CS_COST_PERPI=("TEAM_CONTRACTOR_CS_COST_PERPI","sum"),
        TEAM_CONTRACTOR_C_COST_PERPI=("TEAM_CONTRACTOR_C_COST_PERPI","sum"),
    ).reset_index()
    ag["TOTAL_COST_PERPI"] = ag[
        ["TEAM_COST_PERPI","DEL_TEAM_COST_PERPI","TEAM_CONTRACTOR_CS_COST_PERPI","TEAM_CONTRACTOR_C_COST_PERPI"]
    ].sum(axis=1)
    st.dataframe(ag, use_container_width=True, height=320)

    st.markdown("#### Per-Feature Detail (to spot anomalies)")
    cols_order = [
        "TEAMNAME","ADO_YEAR","ITERATION_NUM","ITERATION_PATH",
        "FEATURE_ID","TITLE","APP_NAME_RAW","EFFORT_POINTS",
        "FEATURES_IN_PI","TEAM_PI_FIXED_COST","TEAM_COST_PERPI",
        "DEL_TEAM_COST_PERPI","TEAM_CONTRACTOR_CS_COST_PERPI","TEAM_CONTRACTOR_C_COST_PERPI"
    ]
    cols_present = [c for c in cols_order if c in df_calc.columns]
    st.dataframe(df_calc[cols_present], use_container_width=True, height=460)

    st.markdown("#### Unmapped ADO Teams (with counts)")
    st.dataframe(unmapped_team, use_container_width=True, height=220)

# =========================================================
# 📦 Bulk Load (ONE sheet): Programs, Vendors, App Groups, Applications, Teams & Invoices
# =========================================================
with tab_bulk:
    st.subheader("Bulk Load (one sheet): Programs, Vendors, App Groups, Applications, Teams & Invoices")
    st.caption("Upload a single sheet that contains the columns for all sections. Map once, preview auto‑generates, then MERGE in a safe order with UUIDs.")

    upl_one = st.file_uploader("Upload workbook (XLSX preferred; CSV allowed)", type=["xlsx","xlsm","xlsb","xls","csv"], key="one_workbook")
    sheet = None
    if upl_one:
        upl_one.seek(0); raw = upl_one.read(); upl_one.seek(0)
        sheets = _list_excel_sheets(raw)
        if sheets:
            sheet = st.selectbox("Worksheet (one sheet for everything)", sheets, index=0, key="one_sheet_select")
        else:
            st.info("No sheet list available (CSV or detection failed). I will parse the first/only sheet.")

        diag = {}
        try:
            df_src = _read_file_any(upl_one, sheet, diag)
            # De-duplicate column labels cleanly for display logic (PyArrow disallows dup names)
            seen = {}
            newcols = []
            for c in df_src.columns:
                key = c
                if key in seen:
                    seen[key] += 1
                    key = f"{c}__{seen[c]}"
                else:
                    seen[key] = 0
                newcols.append(key)
            df_src.columns = newcols

            st.session_state["one_sheet_df"] = df_src
            st.success(f"Parsed {len(df_src)} rows from {upl_one.name}.")
        except Exception as e:
            with st.expander("Diagnostics", expanded=True):
                st.write("**Why it failed**")
                st.exception(e)
                st.write("**Parse attempts**")
                st.json(diag)
            st.error("Could not parse the workbook. Try a clean XLSX/CSV with headers.")
            st.stop()

    # -------------------------
    # Column mapping (single source)
    # -------------------------
    if st.session_state["one_sheet_df"] is not None:
        df_src = st.session_state["one_sheet_df"]
        cols = df_src.columns.tolist()

        def _default_pick(name: str) -> Optional[str]:
            """Best-effort guess by exact/lower/contains."""
            lc = {c.lower(): c for c in cols}
            if name.lower() in lc:
                return lc[name.lower()]
            # contains heuristic
            matches = [c for c in cols if name.lower() in c.lower()]
            return matches[0] if matches else None

        # Initialize mapping once
        if not st.session_state["colmap"]:
            st.session_state["colmap"] = {
                # Programs
                "PROGRAMNAME": _default_pick("PROGRAMNAME") or _default_pick("PROGRAM"),

                # Vendors
                "VENDORNAME": _default_pick("VENDORNAME") or _default_pick("VENDOR"),

                # App Groups
                "GROUPNAME": _default_pick("APP GROUP NAME") or _default_pick("GROUPNAME") or _default_pick("GROUP"),

                # Applications (only APPNAME; group comes from GROUPNAME mapping)
                "APPNAME": _default_pick("APPNAME") or _default_pick("APPLICATION"),

                # Teams (only TEAMNAME; program comes from PROGRAMNAME mapping)
                "TEAMNAME": _default_pick("TEAMNAME") or _default_pick("TEAM"),

                # Invoices (required)
                "AMOUNT_NEXT_YEAR": _default_pick("AMOUNT_NEXT_YEAR") or _default_pick("AMOUNT NEXT YEAR"),
                "AMOUNT": _default_pick("AMOUNT"),
                "FISCAL_YEAR": _default_pick("FISCAL_YEAR") or _default_pick("YEAR"),
                "RENEWAL_MONTH": _default_pick("RENEWAL_MONTH") or _default_pick("RENEWAL MONTH"),

                # Invoices (optional quality-of-life)
                "CONTRACT_ACTIVE": _default_pick("CONTRACT_ACTIVE"),
                "SERIAL_NUMBER": _default_pick("SERIAL_NUMBER"),
                "WORK_ORDER": _default_pick("WORK_ORDER"),
                "COMPANY_CODE": _default_pick("COMPANY_CODE"),
                "COST_CENTER": _default_pick("COST_CENTER"),
                "PRODUCT_OWNER": _default_pick("PRODUCT_OWNER"),
                "NOTES": _default_pick("NOTES"),
            }

        cm = st.session_state["colmap"]

        st.markdown("### Column Mapping")
        c1, c2, c3 = st.columns(3)

        with c1:
            st.markdown("**Program**")
            cm["PROGRAMNAME"] = st.selectbox("PROGRAMNAME (required)", options=["(none)"] + cols,
                                             index=(cols.index(cm["PROGRAMNAME"]) + 1) if cm["PROGRAMNAME"] in cols else 0,
                                             key="map_programname")
            st.markdown("**Vendor**")
            cm["VENDORNAME"] = st.selectbox("VENDORNAME (required)", options=["(none)"] + cols,
                                            index=(cols.index(cm["VENDORNAME"]) + 1) if cm["VENDORNAME"] in cols else 0,
                                            key="map_vendorname")
            st.markdown("**Application Group**")
            cm["GROUPNAME"] = st.selectbox("APP GROUP NAME (required)", options=["(none)"] + cols,
                                           index=(cols.index(cm["GROUPNAME"]) + 1) if cm["GROUPNAME"] in cols else 0,
                                           key="map_groupname")

        with c2:
            st.markdown("**Applications**")
            cm["APPNAME"] = st.selectbox("APPNAME (required)", options=["(none)"] + cols,
                                         index=(cols.index(cm["APPNAME"]) + 1) if cm["APPNAME"] in cols else 0,
                                         key="map_appname")
            st.markdown("**Teams**")
            cm["TEAMNAME"] = st.selectbox("TEAMNAME (required)", options=["(none)"] + cols,
                                          index=(cols.index(cm["TEAMNAME"]) + 1) if cm["TEAMNAME"] in cols else 0,
                                          key="map_teamname")

        with c3:
            st.markdown("**Invoices (required)**")
            cm["AMOUNT_NEXT_YEAR"] = st.selectbox("AMOUNT_NEXT_YEAR", options=["(none)"] + cols,
                                                  index=(cols.index(cm["AMOUNT_NEXT_YEAR"]) + 1) if cm["AMOUNT_NEXT_YEAR"] in cols else 0,
                                                  key="map_inv_amt_next")
            cm["AMOUNT"] = st.selectbox("AMOUNT", options=["(none)"] + cols,
                                        index=(cols.index(cm["AMOUNT"]) + 1) if cm["AMOUNT"] in cols else 0,
                                        key="map_inv_amt")
            cm["FISCAL_YEAR"] = st.selectbox("FISCAL_YEAR", options=["(none)"] + cols,
                                             index=(cols.index(cm["FISCAL_YEAR"]) + 1) if cm["FISCAL_YEAR"] in cols else 0,
                                             key="map_inv_fy")
            cm["RENEWAL_MONTH"] = st.selectbox("RENEWAL_MONTH (1-12)", options=["(none)"] + cols,
                                               index=(cols.index(cm["RENEWAL_MONTH"]) + 1) if cm["RENEWAL_MONTH"] in cols else 0,
                                               key="map_inv_rmonth")

            st.markdown("**Invoices (optional)**")
            for opt_key, label in [
                ("CONTRACT_ACTIVE", "CONTRACT_ACTIVE"),
                ("SERIAL_NUMBER", "SERIAL_NUMBER"),
                ("WORK_ORDER", "WORK_ORDER"),
                ("COMPANY_CODE", "COMPANY_CODE"),
                ("COST_CENTER", "COST_CENTER"),
                ("PRODUCT_OWNER", "PRODUCT_OWNER"),
                ("NOTES", "NOTES"),
            ]:
                cm[opt_key] = st.selectbox(label, options=["(none)"] + cols,
                                           index=(cols.index(cm[opt_key]) + 1) if cm.get(opt_key) in cols else 0,
                                           key=f"map_opt_{opt_key}")

        st.markdown("---")

        # -------------------------
        # Build auto-previews from one sheet using the mapping
        # -------------------------
        def _col(k: str) -> Optional[str]:
            v = cm.get(k)
            return v if v and v != "(none)" and v in df_src.columns else None

        def _safe_num(series: pd.Series) -> pd.Series:
            return pd.to_numeric(series.astype(str).str.replace(",", ".", regex=False), errors="coerce")

        # Programs preview (unique names)
        dfP = pd.DataFrame()
        if _col("PROGRAMNAME"):
            dfP = (
                df_src[[_col("PROGRAMNAME")]].rename(columns={_col("PROGRAMNAME"): "PROGRAMNAME"})
                .assign(PROGRAMNAME=lambda d: d["PROGRAMNAME"].astype(str).str.strip())
                .query("PROGRAMNAME != ''")
                .drop_duplicates(subset=["PROGRAMNAME"])
                .reset_index(drop=True)
            )

        # Vendors preview (unique names)
        dfV = pd.DataFrame()
        if _col("VENDORNAME"):
            dfV = (
                df_src[[_col("VENDORNAME")]].rename(columns={_col("VENDORNAME"): "VENDORNAME"})
                .assign(VENDORNAME=lambda d: d["VENDORNAME"].astype(str).str.strip())
                .query("VENDORNAME != ''")
                .drop_duplicates(subset=["VENDORNAME"])
                .reset_index(drop=True)
            )

        # Groups preview (unique names + vendor shown for preview only)
        dfG = pd.DataFrame()
        if _col("GROUPNAME"):
            tmp = df_src[[_col("GROUPNAME")]].rename(columns={_col("GROUPNAME"): "GROUPNAME"})
            tmp["GROUPNAME"] = tmp["GROUPNAME"].astype(str).str.strip()
            tmp = tmp[tmp["GROUPNAME"] != ""]
            tmp = tmp.drop_duplicates(subset=["GROUPNAME"]).reset_index(drop=True)
            # show TEAMNAME/PROGRAMNAME for context if available (not used as mapping here)
            if _col("TEAMNAME"):
                tmp["TEAMNAME"] = df_src[_col("TEAMNAME")]
            if _col("PROGRAMNAME"):
                tmp["PROGRAMNAME"] = df_src[_col("PROGRAMNAME")]
            # vendor column for preview convenience
            if _col("VENDORNAME"):
                tmp["DEFAULT_VENDORNAME"] = df_src[_col("VENDORNAME")]
            dfG = tmp[["GROUPNAME"] + [c for c in ["TEAMNAME","PROGRAMNAME","DEFAULT_VENDORNAME"] if c in tmp.columns]].drop_duplicates("GROUPNAME").reset_index(drop=True)

        # Applications preview (APPNAME with its group name for context)
        dfA = pd.DataFrame()
        if _col("APPNAME"):
            tmp = df_src[[_col("APPNAME")]].rename(columns={_col("APPNAME"): "APPNAME"})
            tmp["APPNAME"] = tmp["APPNAME"].astype(str).str.strip()
            tmp = tmp[tmp["APPNAME"] != ""]
            if _col("GROUPNAME"):
                tmp["GROUPNAME"] = df_src[_col("GROUPNAME")].astype(str).str.strip()
            dfA = tmp.drop_duplicates(subset=["APPNAME","GROUPNAME"] if "GROUPNAME" in tmp.columns else ["APPNAME"]).reset_index(drop=True)

        # Teams preview (TEAMNAME with its program for context)
        dfT = pd.DataFrame()
        if _col("TEAMNAME"):
            tmp = df_src[[_col("TEAMNAME")]].rename(columns={_col("TEAMNAME"): "TEAMNAME"})
            tmp["TEAMNAME"] = tmp["TEAMNAME"].astype(str).str.strip()
            tmp = tmp[tmp["TEAMNAME"] != ""]
            if _col("PROGRAMNAME"):
                tmp["PROGRAMNAME"] = df_src[_col("PROGRAMNAME")].astype(str).str.strip()
            dfT = tmp.drop_duplicates(subset=["TEAMNAME"]).reset_index(drop=True)

        # Invoices preview (required fields + derived RENEWALDATE)
        dfI = pd.DataFrame()
        missing_invoice_reqs = [k for k in ["AMOUNT_NEXT_YEAR","AMOUNT","FISCAL_YEAR","RENEWAL_MONTH","VENDORNAME"] if not _col(k)]
        if not missing_invoice_reqs:
            out = pd.DataFrame()
            out["AMOUNT_NEXT_YEAR"] = _safe_num(df_src[_col("AMOUNT_NEXT_YEAR")])
            out["AMOUNT"] = _safe_num(df_src[_col("AMOUNT")])
            out["FISCAL_YEAR"] = pd.to_numeric(df_src[_col("FISCAL_YEAR")], errors="coerce").astype("Int64")
            out["RENEWAL_MONTH"] = pd.to_numeric(df_src[_col("RENEWAL_MONTH")], errors="coerce").astype("Int64")

            def _mk_date(row):
                fy = row.get("FISCAL_YEAR")
                m  = row.get("RENEWAL_MONTH")
                try:
                    if pd.isna(fy) or pd.isna(m): return None
                    mm = int(m); yy = int(fy)
                    if mm < 1 or mm > 12: return None
                    return date(yy, mm, 1)
                except Exception:
                    return None

            out["RENEWALDATE"] = out.apply(_mk_date, axis=1)

            # Attach context identifiers by NAME (we'll resolve to IDs later)
            if _col("PROGRAMNAME"): out["PROGRAMNAME"] = df_src[_col("PROGRAMNAME")].astype(str).str.strip()
            if _col("TEAMNAME"): out["TEAMNAME"] = df_src[_col("TEAMNAME")].astype(str).str.strip()
            if _col("GROUPNAME"): out["GROUPNAME"] = df_src[_col("GROUPNAME")].astype(str).str.strip()
            if _col("APPNAME"): out["APPNAME"] = df_src[_col("APPNAME")].astype(str).str.strip()
            out["VENDORNAME"] = df_src[_col("VENDORNAME")].astype(str).str.strip()

            # Optionals
            for k in ["CONTRACT_ACTIVE","SERIAL_NUMBER","WORK_ORDER","COMPANY_CODE","COST_CENTER","PRODUCT_OWNER","NOTES"]:
                if _col(k):
                    out[k] = df_src[_col(k)]

            dfI = out

        # store previews
        st.session_state["previews"] = {"Programs": dfP, "Vendors": dfV, "Groups": dfG, "Apps": dfA, "Teams": dfT, "Invoices": dfI}

        # show previews
        st.markdown("#### Previews")
        p1, p2 = st.columns(2)
        with p1:
            st.markdown("**Programs (unique)**")
            st.dataframe(dfP.head(400), use_container_width=True, height=220)
            st.markdown("**Vendors (unique)**")
            st.dataframe(dfV.head(400), use_container_width=True, height=220)
            st.markdown("**Application Groups (unique)**")
            st.dataframe(dfG.head(400), use_container_width=True, height=240)
        with p2:
            st.markdown("**Applications (unique)**")
            st.dataframe(dfA.head(400), use_container_width=True, height=240)
            st.markdown("**Teams (unique)**")
            st.dataframe(dfT.head(400), use_container_width=True, height=220)
            st.markdown("**Invoices**")
            st.dataframe(dfI.head(400), use_container_width=True, height=260)

        st.markdown("---")

        # -------------------------
        # Upsert ALL in safe order, with UUIDs and name→ID resolution
        # -------------------------
        def _uuid() -> str:
            return str(uuid.uuid4())

        def _name_map(df: pd.DataFrame, key_col: str, id_col: str, name_col: str) -> Dict[str, str]:
            if df.empty: return {}
            m: Dict[str, str] = {}
            for _, r in df.iterrows():
                nm = str(r.get(name_col) or "").strip()
                idv = r.get(id_col)
                if nm and idv:
                    m[nm.upper()] = idv
            return m

        if st.button("⬆️ Upsert ALL (Vendors → Programs → Teams → Groups → Apps → Invoices)", type="primary", key="upsert_all_one_sheet"):
            try:
                # ---------- pre-counts ----------
                def _count(tbl: str) -> int:
                    df = fetch_df(f"SELECT COUNT(*) AS N FROM {tbl}")
                    return int(df.iloc[0]["N"]) if not df.empty else 0

                pre = {
                    "vendors": _count("VENDORS"),
                    "programs": _count("PROGRAMS"),
                    "teams": _count("TEAMS"),
                    "groups": _count("APPLICATION_GROUPS"),
                    "apps": _count("APPLICATIONS"),
                    "invoices": _count("INVOICES"),
                }

                # ---------- Vendors ----------
                inserted_v, updated_v = 0, 0
                if not dfV.empty:
                    existing_vendors = list_vendors()
                    vmap = {str(r.VENDORNAME).strip().upper(): r.VENDORID for _, r in existing_vendors.iterrows()} if not existing_vendors.empty else {}
                    for _, r in dfV.iterrows():
                        vname = str(r["VENDORNAME"]).strip()
                        if not vname:
                            continue
                        existed = vname.upper() in vmap
                        vid = vmap.get(vname.upper()) or str(uuid.uuid4())
                        upsert_vendor(vid, vname)
                        vmap[vname.upper()] = vid
                        inserted_v += 0 if existed else 1
                        updated_v  += 1 if existed else 0
                else:
                    vmap = {str(r.VENDORNAME).strip().upper(): r.VENDORID for _, r in list_vendors().iterrows()} if not list_vendors().empty else {}

                # ---------- Programs ----------
                inserted_p, updated_p = 0, 0
                if not dfP.empty:
                    existing_programs = list_programs()
                    pmap = {str(r.PROGRAMNAME).strip().upper(): r.PROGRAMID for _, r in existing_programs.iterrows()} if not existing_programs.empty else {}
                    for _, r in dfP.iterrows():
                        pname = str(r["PROGRAMNAME"]).strip()
                        if not pname:
                            continue
                        existed = pname.upper() in pmap
                        pid = pmap.get(pname.upper()) or str(uuid.uuid4())
                        upsert_program(pid, pname, owner=None, fte=None)
                        pmap[pname.upper()] = pid
                        inserted_p += 0 if existed else 1
                        updated_p  += 1 if existed else 0
                else:
                    pmap = {str(r.PROGRAMNAME).strip().upper(): r.PROGRAMID for _, r in list_programs().iterrows()} if not list_programs().empty else {}

                # ---------- Teams (uses PROGRAMNAME) ----------
                inserted_t, updated_t = 0, 0
                if not dfT.empty:
                    existing_teams = list_teams()
                    tmap = {str(r.TEAMNAME).strip().upper(): r.TEAMID for _, r in existing_teams.iterrows()} if not existing_teams.empty else {}
                    for _, r in dfT.iterrows():
                        tname = str(r["TEAMNAME"]).strip()
                        if not tname:
                            continue
                        pid = None
                        if "PROGRAMNAME" in r and str(r["PROGRAMNAME"]).strip():
                            pid = pmap.get(str(r["PROGRAMNAME"]).strip().upper())
                        existed = tname.upper() in tmap
                        tid = tmap.get(tname.upper()) or str(uuid.uuid4())
                        upsert_team(
                            team_id=tid,
                            name=tname,
                            program_id=pid,
                            team_fte=None, delivery_team_fte=None,
                            contractor_c_fte=None, contractor_cs_fte=None
                        )
                        tmap[tname.upper()] = tid
                        inserted_t += 0 if existed else 1
                        updated_t  += 1 if existed else 0
                else:
                    tmap = {str(r.TEAMNAME).strip().upper(): r.TEAMID for _, r in list_teams().iterrows()} if not list_teams().empty else {}

                # ---------- Groups (uses TEAMNAME for context, default vendor) ----------
                inserted_g, updated_g = 0, 0
                if not dfG.empty:
                    tmap = {str(r.TEAMNAME).strip().upper(): r.TEAMID for _, r in list_teams().iterrows()} or tmap
                    existing_groups = list_application_groups()
                    gmap = {str(r.GROUPNAME).strip().upper(): r.GROUPID for _, r in existing_groups.iterrows()} if not existing_groups.empty else {}
                    vmap = {str(r.VENDORNAME).strip().upper(): r.VENDORID for _, r in list_vendors().iterrows()} or vmap

                    for _, r in dfG.iterrows():
                        gname = str(r["GROUPNAME"]).strip()
                        if not gname:
                            continue
                        teamid = None
                        if "TEAMNAME" in r and str(r["TEAMNAME"]).strip():
                            teamid = tmap.get(str(r["TEAMNAME"]).strip().upper())
                        default_vendor_id = None
                        if "DEFAULT_VENDORNAME" in r and str(r["DEFAULT_VENDORNAME"]).strip():
                            default_vendor_id = vmap.get(str(r["DEFAULT_VENDORNAME"]).strip().upper())

                        existed = gname.upper() in gmap
                        gid = gmap.get(gname.upper()) or str(uuid.uuid4())
                        upsert_application_group(gid, gname, team_id=(teamid or ""), default_vendor_id=default_vendor_id, owner=None)
                        gmap[gname.upper()] = gid
                        inserted_g += 0 if existed else 1
                        updated_g  += 1 if existed else 0
                else:
                    gmap = {str(r.GROUPNAME).strip().upper(): r.GROUPID for _, r in list_application_groups().iterrows()} if not list_application_groups().empty else {}

                # ---------- Applications (APPNAME + GROUPNAME link) ----------
                inserted_a, updated_a = 0, 0
                if not dfA.empty:
                    cur_groups = list_application_groups()
                    gmap = {str(r.GROUPNAME).strip().upper(): r.GROUPID for _, r in cur_groups.iterrows()} or gmap

                    existing_apps = list_applications()
                    amap = {str(r.APPLICATIONNAME).strip().upper(): r.APPLICATIONID for _, r in existing_apps.iterrows()} if not existing_apps.empty else {}

                    for _, r in dfA.iterrows():
                        aname = str(r["APPNAME"]).strip()
                        if not aname:
                            continue
                        gid = None
                        if "GROUPNAME" in r and str(r["GROUPNAME"]).strip():
                            gid = gmap.get(str(r["GROUPNAME"]).strip().upper())

                        existed = aname.upper() in amap
                        aid = amap.get(aname.upper()) or str(uuid.uuid4())
                        upsert_application_instance(
                            application_id=aid,
                            group_id=(gid or ""),
                            application_name=aname,
                            add_info=None,
                            vendor_id=None
                        )
                        amap[aname.upper()] = aid
                        inserted_a += 0 if existed else 1
                        updated_a  += 1 if existed else 0
                else:
                    amap = {}
                    cur_apps = list_applications()
                    if not cur_apps.empty:
                        for _, r in cur_apps.iterrows():
                            nm = str(r.get("APPLICATIONNAME") or "").strip()
                            aid = r.get("APPLICATIONID")
                            if nm and aid:
                                amap[nm.upper()] = aid

                # ---------- Invoices (name-based resolution) ----------
                created_i = 0
                skipped_i = [0]  # Use a list for mutability
                skipped_reasons: Dict[str, int] = {}

                def _skip(reason: str):
                    skipped_i[0] += 1
                    skipped_reasons[reason] = skipped_reasons.get(reason, 0) + 1

                if not dfI.empty:
                    # Refresh maps
                    pmap = {str(r.PROGRAMNAME).strip().upper(): r.PROGRAMID for _, r in list_programs().iterrows()} or pmap
                    tmap = {str(r.TEAMNAME).strip().upper(): r.TEAMID for _, r in list_teams().iterrows()} or tmap
                    gmap = {str(r.GROUPNAME).strip().upper(): r.GROUPID for _, r in list_application_groups().iterrows()} or gmap
                    vmap = {str(r.VENDORNAME).strip().upper(): r.VENDORID for _, r in list_vendors().iterrows()} or vmap

                    amap = {}
                    cur_apps2 = list_applications()
                    if not cur_apps2.empty:
                        for _, r0 in cur_apps2.iterrows():
                            nm = str(r0.get("APPLICATIONNAME") or "").strip()
                            aid = r0.get("APPLICATIONID")
                            if nm and aid:
                                amap[nm.upper()] = aid

                    for _, r in dfI.iterrows():
                        vname = str(r.get("VENDORNAME") or "").strip()
                        fy = r.get("FISCAL_YEAR")
                        rmonth = r.get("RENEWAL_MONTH")
                        amt = r.get("AMOUNT")
                        amt_next = r.get("AMOUNT_NEXT_YEAR")

                        if not vname:            _skip("missing vendor");      continue
                        if pd.isna(fy):          _skip("missing fiscal year"); continue
                        if pd.isna(rmonth):      _skip("missing renewal month"); continue
                        if pd.isna(amt):         _skip("missing amount");      continue
                        if pd.isna(amt_next):    _skip("missing amount_next_year"); continue

                        vendorid_at_booking = vmap.get(vname.upper())
                        if not vendorid_at_booking:
                            _skip("vendor not found"); continue

                        teamid = None
                        if "TEAMNAME" in r and str(r["TEAMNAME"]).strip():
                            teamid = tmap.get(str(r["TEAMNAME"]).strip().upper())
                        groupid = None
                        if "GROUPNAME" in r and str(r["GROUPNAME"]).strip():
                            groupid = gmap.get(str(r["GROUPNAME"]).strip().upper())
                        appid = None
                        if "APPNAME" in r and str(r["APPNAME"]).strip():
                            appid = amap.get(str(r["APPNAME"]).strip().upper())

                        try:
                            d_renew = date(int(fy), int(rmonth), 1)
                        except Exception:
                            _skip("invalid renewal date"); continue

                        inv_id = str(uuid.uuid4())
                        upsert_invoice(
                            invoice_id=inv_id,
                            application_id=str(appid or ""),
                            team_id=str(teamid or ""),
                            renewal_date=d_renew,
                            amount=float(amt) if not pd.isna(amt) else None,
                            status="Planned",
                            fiscal_year=int(fy) if not pd.isna(fy) else None,
                            product_owner=(str(r.get("PRODUCT_OWNER")) if pd.notna(r.get("PRODUCT_OWNER")) else None),
                            amount_next_year=float(amt_next) if not pd.isna(amt_next) else None,
                            contract_active=bool(r.get("CONTRACT_ACTIVE")) if pd.notna(r.get("CONTRACT_ACTIVE")) else True,
                            company_code=(str(r.get("COMPANY_CODE")) if pd.notna(r.get("COMPANY_CODE")) else None),
                            cost_center=(str(r.get("COST_CENTER")) if pd.notna(r.get("COST_CENTER")) else None),
                            serial_number=(str(r.get("SERIAL_NUMBER")) if pd.notna(r.get("SERIAL_NUMBER")) else None),
                            work_order=(str(r.get("WORK_ORDER")) if pd.notna(r.get("WORK_ORDER")) else None),
                            agreement_number=None,
                            contract_due=None,
                            service_type=None,
                            notes=(str(r.get("NOTES")) if pd.notna(r.get("NOTES")) else None),
                            group_id=str(groupid or ""),
                            programid_at_booking=None,
                            vendorid_at_booking=str(vendorid_at_booking or ""),
                            groupid_at_booking=str(groupid or ""),
                            rollover_batch_id=None,
                            rolled_over_from_year=None,
                            invoice_type="Recurring Invoice",
                        )
                        created_i += 1

                # ---------- post-counts & summary ----------
                post = {
                    "vendors": _count("VENDORS"),
                    "programs": _count("PROGRAMS"),
                    "teams": _count("TEAMS"),
                    "groups": _count("APPLICATION_GROUPS"),
                    "apps": _count("APPLICATIONS"),
                    "invoices": _count("INVOICES"),
                }

                st.success(
                    "Upsert complete ✅\n\n"
                    f"- Vendors: **{inserted_v} inserted**, **{updated_v} updated** (total: {post['vendors']} | was {pre['vendors']})\n"
                    f"- Programs: **{inserted_p} inserted**, **{updated_p} updated** (total: {post['programs']} | was {pre['programs']})\n"
                    f"- Teams: **{inserted_t} inserted**, **{updated_t} updated** (total: {post['teams']} | was {pre['teams']})\n"
                    f"- App Groups: **{inserted_g} inserted**, **{updated_g} updated** (total: {post['groups']} | was {pre['groups']})\n"
                    f"- Applications: **{inserted_a} inserted**, **{updated_a} updated** (total: {post['apps']} | was {pre['apps']})\n"
                    f"- Invoices: **{created_i} inserted**, **{skipped_i[0]} skipped** (total: {post['invoices']} | was {pre['invoices']})"
                )

                if skipped_i:
                    with st.expander("Why invoices were skipped", expanded=False):
                        for reason, n in sorted(skipped_reasons.items(), key=lambda x: -x[1]):
                            st.write(f"- {reason}: **{n}**")

            except Exception as e:
                st.error(f"Upsert failed: {e}")

    else:
        st.info("Upload your workbook above to proceed.")
